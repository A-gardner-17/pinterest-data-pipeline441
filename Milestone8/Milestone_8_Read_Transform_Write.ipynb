{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cdd592e-845f-4a4a-b2cf-54a50b52e44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Milestone 8 Read Transform Write Kinesis Stream on Databricks##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb09a1fc-7395-4a78-818b-3cb70cb3f007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5538f2-9704-4604-9b0d-1d9106a684fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.formatCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cbfd86-e849-4a37-8aa6-8fb70a986397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','Kinesis-Prod-Stream') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "# Define geo structure\n",
    "geo_struct = StructType([\n",
    "    StructField(\"ind\", IntegerType(),True),\n",
    "    StructField(\"timestamp\", StringType(),True),\n",
    "    StructField(\"latitude\", StringType(),True),\n",
    "    StructField(\"longitude\", StringType(),True),\n",
    "    StructField(\"country\", StringType(),True)                        \n",
    "])\n",
    "\n",
    "df_geo = df.filter(df.partitionKey == \"streaming-57e94de2a910-geo\")\n",
    "# Decode JSON Data\n",
    "df_geo = df_geo.select(from_json(col(\"data\").cast(\"string\"), geo_struct).alias(\"parsed_value\")).select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c8fef4-7da1-403e-929d-8fb2b4f254ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define pin structure\n",
    "pin_struct = StructType([\n",
    "    StructField(\"index\", IntegerType(),True),\n",
    "    StructField(\"unique_id\", StringType(),True),\n",
    "    StructField(\"title\", StringType(),True),\n",
    "    StructField(\"description\", StringType(),True),\n",
    "    StructField(\"poster_name\", StringType(),True),\n",
    "    StructField(\"follower_count\", StringType(),True),\n",
    "    StructField(\"tag_list\", StringType(),True),\n",
    "    StructField(\"is_image_or_video\", StringType(),True),\n",
    "    StructField(\"image_src\", StringType(),True),\n",
    "    StructField(\"downloaded\", StringType(),True),\n",
    "    StructField(\"save_location\", StringType(),True),\n",
    "    StructField(\"category\", StringType(),True)\n",
    "])\n",
    "df_pin = df.filter(df.partitionKey == \"streaming-57e94de2a910-pin\")\n",
    "# Decode JSON Data\n",
    "df_pin = df_pin.select(from_json(col(\"data\").cast(\"string\"), pin_struct).alias(\"parsed_value\")).select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4c051e-deec-4fe3-ac6f-822061fe639d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define user structure\n",
    "user_struct = StructType([\n",
    "    StructField(\"ind\", IntegerType(),True),\n",
    "    StructField(\"first_name\", StringType(),True),\n",
    "    StructField(\"last_name\", StringType(),True),\n",
    "    StructField(\"age\", IntegerType(),True),\n",
    "    StructField(\"date_joined\", StringType(),True)\n",
    "])\n",
    "df_user = df.filter(df.partitionKey == \"streaming-57e94de2a910-user\")\n",
    "# Decode JSON Data\n",
    "df_user = df_user.select(from_json(col(\"data\").cast(\"string\"), user_struct).alias(\"parsed_value\")).select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192ad33c-82a4-48ac-a8d7-d7529ff61add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean pin data\n",
    "\n",
    "from pyspark.sql.functions import when, col, lit, regexp_replace\n",
    "\n",
    "# Replace empty entries and entries with no relevant data in each column with Nones\n",
    "replace_values = [\"\", \"N/A\", \"null\", \"No description available\", \"No description available Story format\", \"User Info Error\", \"Image src error.\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"No Title Data Available\"]\n",
    "\n",
    "# Replace empty and irrelevant entries with None for each column\n",
    "df_pin = df_pin.select([\n",
    "    when(col(c).isin(replace_values) | (col(c) == \"\"), lit(None)).otherwise(col(c)).alias(c)\n",
    "    for c in df_pin.columns\n",
    "])\n",
    "\n",
    "# Filter rows where 'downloaded' is 1 or 0\n",
    "df_pin = df_pin.filter((col(\"downloaded\") == 1) | (col(\"downloaded\") == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d021ca9-c13d-491c-b107-395cc037a6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Standardize follower_count values\n",
    "df_pin = df_pin.withColumn(\n",
    "    \"follower_count\",\n",
    "    # Replace 'k' with '000' and 'M' with '000000'\n",
    "    when(col(\"follower_count\").endswith(\"k\"), regexp_replace(col(\"follower_count\"), \"k\", \"000\").cast(\"double\"))\n",
    "    .when(col(\"follower_count\").endswith(\"M\"), regexp_replace(col(\"follower_count\"), \"M\", \"000000\").cast(\"double\"))\n",
    "    .otherwise(col(\"follower_count\").cast(\"double\"))  # Handle numeric values like '25'\n",
    ")\n",
    "\n",
    "# Step 2: Drop rows where follower_count is null or not valid\n",
    "df_pin = df_pin.filter(col(\"follower_count\").isNotNull())\n",
    "\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast(IntegerType()))\n",
    "\n",
    "# rename the index column to ind\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f733c459-e147-4882-bece-614c99d34578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# column order\n",
    "column_order = [\n",
    "    \"ind\",\n",
    "    \"unique_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"follower_count\",\n",
    "    \"poster_name\",\n",
    "    \"tag_list\",\n",
    "    \"is_image_or_video\",\n",
    "    \"image_src\",\n",
    "    \"save_location\",\n",
    "    \"category\"\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_pin = df_pin.select(column_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36227cc-ab6b-4f96-8032-9180fa24ff1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f2ed8d7b400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "table_name = \"57e94de2a910_pin_table\" \n",
    "df_pin.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/57e94de2a910_pin/\") \\\n",
    "    .table(table_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b1a12a-30dd-479c-8d7d-121fb643a8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col, to_timestamp\n",
    "\n",
    "# Create a new column as an array of 'column1' and 'column2'\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop the latitude and longitude columns\n",
    "df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# Filter out rows where conversion failed (timestamp is null)\n",
    "df_geo = df_geo.filter(df_geo[\"timestamp\"].isNotNull())\n",
    "\n",
    "# New column order\n",
    "column_order = [\n",
    "    \"ind\",\n",
    "    \"country\",\n",
    "    \"coordinates\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_geo = df_geo.select(column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0a437e-536c-4957-a6b2-9712057bbded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f2ec8bc87c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "table_name = \"57e94de2a910_geo_table\" \n",
    "df_geo.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/57e94de2a910_geo/\") \\\n",
    "    .table(table_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820817-2dbd-4aff-b97d-56c44218eef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, col\n",
    "\n",
    "# Create a new column username that concatenates the information found in the first_name and last_name columns\n",
    "df_user = df_user.withColumn(\"username\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "\n",
    "# Drop the latitude and longitude columns\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the date joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn(\"date_joined\", to_timestamp(col(\"date_joined\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# Filter out rows where conversion failed (timestamp_column is null)\n",
    "df_user = df_user.filter(df_user[\"date_joined\"].isNotNull())\n",
    "\n",
    "# New column order\n",
    "column_order = [\n",
    "    \"ind\",\n",
    "    \"username\",\n",
    "    \"age\",\n",
    "    \"date_joined\"\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_user = df_user.select(column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ff9695-6041-4f3f-b4d5-fadb35f4487d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f2ec86a17b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "table_name = \"57e94de2a910_user_table\" \n",
    "df_user.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/57e94de2a910_user/\") \\\n",
    "    .table(table_name) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8510925054773310,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Milestone 8 - Read, Transform, Write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}